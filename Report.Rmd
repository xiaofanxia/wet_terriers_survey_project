---
title: "Project Report"
author: "Tim, Xiaofan, Yanwen, Jingning"
date: "11/20/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
library(lavaan)
library(magrittr)
library(tidyr)
library(knitr)
library(abind)
library(kableExtra)
survey <- read.csv("survey.csv")
```

# Introduction
Client is investigating how foreign language teachers feel about and utilize methods from the Teacher Effectiveness for Language Learning (TELL), and seeking advice about how to improving the survey.

Our purpose for our client in this project: 
1.  A lot of people don’t answer the survey because it’s long. Can we reduce the number of questions?
2.  Is the survey currently answering the research questions?


# EDA & Conerns

## Data Structure

We are provided the data in an excel file with 6 spreadsheets including one sheet of notes, one sheet of personal information and 4 sheets of Teacher Effectiveness for Language Learning (TELL) framework survey questions. The dataset of personal information contains questions regarding respondents' teaching language and education background. The dataset of Teacher Effectiveness for Language Learning (TELL) framework survey contains around 200 questions asking about respondents' attitudes of contribution and confidence towards each practice in the framework. There are 4 domains of Teacher Effectiveness for Language Learning (TELL) survey questions: planning, learning experience, learning tools, and performance & feedback. Each domain contains several subdomains and each subdomain contains a different number of questions. For this project, we focus on reducing the number of questions in the dataset of Teacher Effectiveness for Language Learning (TELL) framework survey.

## EDA 

We conduct a basic Exploratory Data Analysis (EDA) for this project. Firstly, We focus on the time for respondents to complete this survey.

```{r hist of user review counts,fig.height=8,fig.width=7, message=FALSE, echo=FALSE}
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
survey <- read.csv("survey.csv")
#Uniform newest part time into POSIX objects:
survey$Start.Date <- strptime(survey$Start.Date, format = "%m/%d/%Y %H:%M:%S")
survey$End.Date <- strptime(survey$End.Date, format = "%m/%d/%Y %H:%M:%S")
#Time people used to finish the survey:
survey$time <- as.numeric(round(abs(difftime(survey$End.Date, survey$Start.Date, units = "mins")),2))
#Count number of NA values in rows:
survey$num <- apply(survey, MARGIN = 1, FUN=function(x) length(x[is.na(x)]))
#graph the plot:
# graph <- ggplot(survey, aes(x=time, y=num)) +
#   geom_point(na.rm = TRUE)
# graph <- graph + labs(x = "Time people used in survey(min)", y = "Number of questions they didn't answer") 
# graph
time <- as.data.frame(na.omit(survey$time))
time$num <- survey$num[1:74]
colnames(time)[1] <- "Time people used(minutes)"
colnames(time)[2] <- "# of unanswered questions"
par(mfrow=c(2,4))
session1 <- filter(time,`Time people used(minutes)` <= 15)
session2 <- filter(time, `Time people used(minutes)` > 15 & `Time people used(minutes)` <= 30)
session3 <- filter(time, `Time people used(minutes)` > 30 & `Time people used(minutes)` <= 60)
session4 <- filter(time, `Time people used(minutes)` > 60 & `Time people used(minutes)` <= 1000)
session5 <- filter(time, `Time people used(minutes)` > 1000 & `Time people used(minutes)` <= 1600)
#From 0-15min:
p1 <- ggplot(session1, aes(x=`Time people used(minutes)`, y=`# of unanswered questions`)) + 
  geom_bar(stat = "identity") + ggtitle("Time used in 0-15 minutes") 
#From 15-30min:
p2 <- ggplot(session2, aes(x=`Time people used(minutes)`, y=`# of unanswered questions`)) + 
  geom_bar(stat = "identity") + ggtitle("Time used in 16-30 munites")
#From 31-60min:
p3 <- ggplot(session3, aes(x=`Time people used(minutes)`, y=`# of unanswered questions`)) + 
  geom_bar(stat = "identity") + ggtitle("Time used in 31-60 munites")
#From 61-120min:
p4 <- ggplot(session4, aes(x=`Time people used(minutes)`, y=`# of unanswered questions`)) + 
  geom_bar(stat = "identity") + ggtitle("Time used in 61-400 munites")
#From 121-300min:
p5 <- ggplot(session5, aes(x=`Time people used(minutes)`, y=`# of unanswered questions`)) + 
  geom_bar(stat = "identity") + ggtitle("Time used in 401-1600 munites")
plot_grid(p1, p2, p3, p4, p5, nrow = 3, ncol = 2)
```


## Data Cleaning


## Concerns

We come up with several concerns after the initial EDA. Firstly, the observations we can use in the analysis are very limited since there are many N/A in the dataset. Secondly, some respondents seem like choosing the same answer through the whole survey and if we identify these answers as non-valid, then our sample size would become even smaller. With this limited sample size, the accuracy and reference of results from our subsequent analysis could be affected.

# Method we used 

We will use Confirmatory Factor Analysis (CFA) to reduce the survey questions number.  CFA is a special form of factor analysis and mostly used in social science research. It is used to check whether measures of a construct are consistent with a researcher's understanding of the nature of that construct.  
Here we will use CFA to see if there are survey questions equivalent to each other so we can reduce those repeated questions. We will analysis each subdomain separately and will only consider the problems regarding confidence or not. Within each subdomain, there will be several questions and our null hypothesis is that all survey questions are identical to each other. Then our alternative hypothesis is that the questions are not all equal.  
We will focus on the p value result we have from CFA and we will take a p value larger than 0.05 to reject our null hypothesis. When we are not able to reject our null hypothesis, we will look at our factor loading to check the correlations between questions. Then we will fit new model by dropping question with lowest factor loading and see if we will reject our null hypothesis now. We will keep doing this until we have a subdomain with an acceptable p value, which gives us a set of survey questions are not identical to each other.


# Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
rowclean <- function(filename) {
  data <- read.csv(paste0(filename),na.string=c("","NA"))
  rownum <- max(which(is.na(data[2,])==F))
  clean <- data[,1:rownum]
  colnames(clean) = NULL
  names1 <- clean[1,1:5]
  names2 <- clean[2,6:rownum]
  names<- cbind(names1, names2)
  clean <- clean[3:nrow(clean),]
  colnames(clean) <- c(t(names))
  return(clean)
}
filenames <- list.files()
selectfiles <- filenames[grep("TELL Statements",filenames,perl=FALSE)]
resultlist <- vector("list",length(selectfiles))
for(i in 1:length(selectfiles)) {
  resultlist[[i]] <- rowclean(selectfiles[i])
}
aggregate <- abind(resultlist, along=2,force.array=F)
personinfo <- aggregate[,1:5]
surveyanswers <- aggregate[,-which(names(aggregate) %in% names(personinfo))]
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("LT1a _Confidence",colnames(surveyanswers)))] <- "LT1a_Confidence"
colnames(surveyanswers)[c(grep("LT5c_Contributes.1",colnames(surveyanswers)))] <- "LT56c_Confidence"
data.LT <- surveyanswers[,c(grep("LT",colnames(surveyanswers)))]
data.LT <- data.LT[,c(grep("Confidence",colnames(data.LT)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
LT.ordered <- data.LT
LT.numer <- data.LT
for (i in 1:ncol(data.LT)) {
  LT.ordered[,i] <- factor(data.LT[,i],levels=ans)
  LT.numer[,i] <- as.numeric(LT.ordered[,i])
}
colnames(LT.numer) <- colnames(data.LT)
```

## Learning Tool Domain Analysis
For Learning Tools table in TELL Statements, we numeric character answers of LT 1a~5c Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LT1 has 3 variables: LT1a_Confidence, LT1b_Confidence, LT1c_Confidence).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results:
# First subdomain:
```{r echo=FALSE, message=FALSE, warning=FALSE}
LT1 <- 'lt1 =~ aa*LT1a_Confidence + aa*LT1b_Confidence + LT1c_Confidence '
fit1 <- cfa(LT1,data=LT.numer,std.lv=T)
summary(fit1,standardized=T)
fitMeasures(fit1)
modificationIndices(fit1)
parameterEstimates(fit1,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.741>0.05. Factor loadings: a-0.488;b-0.472;c-0.722. cfi=1;ltli=1.607.
```

Since p-value of the first subdomain is 0.741 > 0.05, there is no need to make any change in the first subdomain and we can save all questions. 

# Second subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT2 <- 'lt2 =~ aa*LT2a_Confidence + aa*LT2b_Confidence + LT2c_Confidence'
fit2 <- cfa(LT2,data=LT.numer,std.lv=T)
summary(fit2,standardized=T)
fitMeasures(fit2)
modificationIndices(fit2)
parameterEstimates(fit2,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.953>0.05. Factor loadings: a-0.587;b-0.603;c-0.817. cfi=1; tli=1.234.
```

Since p-value of the second subdomain is 0.953 > 0.05, there is no need to make any change in the second subdomain and we can save all questions. 

# Third subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT3 <- 'lt3 =~ LT3a_Confidence + LT3b_Confidence + LT3c_Confidence + LT3d_Confidence'
fit3 <- cfa(LT3,data=LT.numer,std.lv=T)
summary(fit3,standardized=T)
fitMeasures(fit3)
modificationIndices(fit3)
parameterEstimates(fit3,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Initial p-value with 4 questions is 0.008. 
```

Since p-value of the third subdomain is 0.008 < 0.05, and question "LT3c_Confidence" has the lowest factor loading 0.604, we drop "LT3_c_Confidence" and then remodel the third subdomain. 

```{r echo=FALSE, message=FALSE, warning=FALSE }
LT3_new <- 'lt3 =~ LT3a_Confidence + aa*LT3b_Confidence + aa*LT3d_Confidence'
fit3new <- cfa(LT3_new,data=LT.numer,std.lv=T)
summary(fit3new,standardized=T)
fitMeasures(fit3new)
modificationIndices(fit3new)
parameterEstimates(fit3new,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Initial p-value with 4 questions is 0.008. 
#After dropping question c, the p-value=0.897. Factor loadings:a-0.959;b-0.802;d-0.629. cfi=1;tli=1.087.
```

After we remodel the third subdomain, the p-value of third domain is 0.897 > 0.05. Then we can save all the remaining questions in the third subdomain ("LT3a_Confidence", "LT3b_Confidence", "LT3d_Confidence"). 

# Fourth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT4 <- 'lt4 =~ LT4a_Confidence + aa*LT4b_Confidence + aa*LT4c_Confidence'
fit4 <- cfa(LT4,data=LT.numer,std.lv=T)
summary(fit4,standardized=T)
fitMeasures(fit4)
modificationIndices(fit4)
parameterEstimates(fit4,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.899. Factor loadings: a-0.469;b-1.012;c-0.886. cfi=1; tli=1.06.
```

Since p-value of the fourth subdomain is 0.899 > 0.05, there is no need to make any change in the fourth subdomain and we can save all questions. 

# Fifth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT5 <- 'lt5 =~ aa*LT5a_Confidence + LT5b_Confidence + aa*LT5c_Confidence'
fit5 <- cfa(LT5,data=LT.numer,std.lv=T)
summary(fit5,standardized=T)
fitMeasures(fit5)
modificationIndices(fit5)
parameterEstimates(fit5,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.379. Factor loadings: a-0.618;b-0.844;c-0.532. cfi=1; tli=1.055.
```

Since p-value of the fifth subdomain is 0.379 > 0.05, there is no need to make any change in the fifth subdomain and we can save all questions. 

## PER & FEEDBACK Domain Analysis
For PER&FEEDBACK table in TELL Statements, I numeric character answers of PF 1a~5c Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain whose variables should greater than 2 (ex: PF1 has 5 variables: PF1a_Confidence, PF1b_Confidence, PF1c_Confidence,PF1d_Confidence and PF1e_Confidence), or the P-value of that model will become NA. And we get an exception in PF table: PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we have an available P-value. Second, we find factor loadings of each variables in each subdomain and record them. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again.
Following are detailed results   
# First subdomain:
```{r, echo=FALSE}
library(tidyverse)
library(lavaan)
library(magrittr)
library(kableExtra)
rowclean <- function(filename) {
  data <- read.csv(paste0(filename),na.string=c("","NA"))
  rownum <- max(which(is.na(data[2,])==F))
  clean <- data[,1:rownum]
  colnames(clean) = NULL
  names1 <- clean[1,1:5]
  names2 <- clean[2,6:rownum]
  names<- cbind(names1, names2)
  clean <- clean[3:nrow(clean),]
  colnames(clean) <- c(t(names))
  return(clean)
}
filenames <- list.files()
selectfiles <- filenames[grep("TELL Statements",filenames,perl=FALSE)]
resultlist <- vector("list",length(selectfiles))
for(i in 1:length(selectfiles)) {
  resultlist[[i]] <- rowclean(selectfiles[i])
}
aggregate <- abind(resultlist, along=2,force.array=F)
personinfo <- aggregate[,1:5]
surveyanswers <- aggregate[,-which(names(aggregate) %in% names(personinfo))]
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("PF1a _Confidence",colnames(surveyanswers)))] <- "PF1a_Confidence"
colnames(surveyanswers)[c(grep("PF5c_Contributes.1",colnames(surveyanswers)))] <- "PF5c_Confidence"
data.PF <- surveyanswers[,c(grep("PF",colnames(surveyanswers)))]
data.PF <- data.PF[,c(grep("Confidence",colnames(data.PF)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
PF.ordered <- data.PF
PF.number <- data.PF
for (i in 1:ncol(data.PF)) {
  PF.ordered[,i] <- factor(data.PF[,i],levels=ans)
  PF.number[,i] <- as.numeric(PF.ordered[,i])
}
colnames(PF.number) <- colnames(data.PF)
# First subdomain
m1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence + PF1e_Confidence'
fit1 <- cfa(m1,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit1,standardized=T)
#P-value = 0.008
parameterEstimates(fit1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.609; 0.830; 0.946; 0.915; 0.587
```


Since p-value of first subdomain is 0.008 < 0.05, and the factor loadings of "PF1e_Confidence" is lowest, thus, we try to drop it from the first subdomain: 
```{r, echo=FALSE}
m1.1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence'
fit1.1 <- cfa(m1.1,data = PF.number, std.lv=TRUE)
summary(fit1.1,standardized=T)
```
P-value = 0.967 > 0.05, thus we do not need to change any more on the first subdomain.

# Second subdomain:
```{r, echo=FALSE}
# Second subdomain
m2 <- 'PF2 =~ PF2a_Confidence + PF2b_Confidence + PF2c_Confidence + PF2d_Confidence + PF2e_Confidence'
fit2 <- cfa(m2,data = PF.number, std.lv=TRUE)
summary(fit2,standardized=T)
#P-value = 0.013
parameterEstimates(fit2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.587; 0.905; 0.558; 0.901; 0.896
```

Since p-value of first subdomain is 0.013 < 0.05, and the factor loadings of "PF2c_Confidence" is lowest, thus, we try to drop it from the second subdomain:
```{r, echo=FALSE}
m2.1 <- m2 <- 'PF2 =~ PF2a_Confidence + PF2b_Confidence + PF2d_Confidence + PF2e_Confidence'
fit2.1 <- cfa(m2.1,data = PF.number, std.lv=TRUE)
summary(fit2.1,standardized=T)
```
P-value = 0.459 > 0.05, thus we can stay here for the second subdomain.


# Third subdomain:
```{r, echo=FALSE}
m3 <- 'PF3 =~ PF3a_Confidence + PF3b_Confidence + PF3c_Confidence + PF3d_Confidence + PF3e_Confidence'
fit3 <- cfa(m3,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit3,standardized=T)
#P-value = 0.712 
parameterEstimates(fit3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.485; 0.838; 0.746; 0.767; 0.588
```
Since p-value > 0.05, the third subdomain is ok, no longer to remodel it.


# Fourth subdomain:
PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we can get an available P-value.
```{r, echo=FALSE}
m4 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence + PF5c_Confidence'
fit4 <- cfa(m4,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit4,standardized=T)
#P-value = 0.012 
parameterEstimates(fit4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.888; 0.835; 0.757; 0.759; 0.567
```

Since P-value is 0.012 < 0.05, and the lowest factor loading is "PF5c_Confidence", thus we try to drop it from the subdomain:
```{r, echo=FALSE}
m4.1 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence'
fit4.1 <- cfa(m4.1,data = PF.number, std.lv=TRUE)
summary(fit4.1,standardized=T)
```
P-value is 0.362 > 0.05, thus no longer remodel this subdomain.

## Learning Experience Domain Analysis
For learning experience table in TELL Statements, we numeric character answers of LE 1a~6d Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LE1 has 5 variables: LE1a_Confidence, LE1b_Confidence, LE1c_Confidence,LE1d_Confidence and LE1e_Confidence).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results

## First subdomian
```{r,echo=F}
rowclean <- function(filename) {
  data <- read.csv(paste0(filename),na.string=c("","NA"))
  rownum <- max(which(is.na(data[2,])==F))
  clean <- data[,1:rownum]
  colnames(clean) = NULL
  names1 <- clean[1,1:5]
  names2 <- clean[2,6:rownum]
  names<- cbind(names1, names2)
  clean <- clean[3:nrow(clean),]
  colnames(clean) <- c(t(names))
  return(clean)
}
filenames <- list.files()
selectfiles <- filenames[grep("TELL Statements",filenames,perl=FALSE)]
resultlist <- vector("list",length(selectfiles))
for(i in 1:length(selectfiles)) {
  resultlist[[i]] <- rowclean(selectfiles[i])
}
aggregate <- abind(resultlist, along=2,force.array=F)
personinfo <- aggregate[,1:5]
surveyanswers <- aggregate[,-which(names(aggregate) %in% names(personinfo))]
survey <- cbind(personinfo,surveyanswers)

PL.factor <- select(survey,'LE1a_Contributes':'LE6d_Confidence')
PL.factor <-lapply(PL.factor,function(x) as.numeric(as.factor(x)))
PL.factor <- as.data.frame(PL.factor)
LE1model<-'LE1 =~LE1a_Confidence+LE1b_Confidence+LE1c_Confidence+LE1d_Confidence+LE1e_Confidence'
fitLE1 <- cfa(LE1model,data=PL.factor)
summary(fitLE1)
parameterEstimates(fitLE1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

The p-value of this subdomian is 0.762, so we will keep all the questions in this subdomian.

## Second Subdomain
```{r,echo=F}
LE2model<-'LE2 =~LE2a_Confidence+LE2b_Confidence+LE2c_Confidence+LE2d_Confidence+LE2e_Confidence+LE2f_Confidence'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")

```

In the second subdomain, the p-value is 0.028<0.05, so we will drop the question LE2a to see how the model will be.
```{r,echo=F}
LE2model<-'LE2 =~LE2b_Confidence+LE2c_Confidence+LE2d_Confidence+LE2e_Confidence+LE2f_Confidence'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

After dropping the LE2a, we have a p value of 0.9>0.05. So we will keep all the other questions.

## Third Subdomain
```{r,echo=F}
LE3model<-'LE3 =~LE3a_Confidence+LE3b_Confidence+LE3c_Confidence+LE3d_Confidence+LE3e_Confidence+LE3f_Confidence+LE3g_Confidence'
fitLE3 <- cfa(LE3model,data=PL.factor)
summary(fitLE3)
parameterEstimates(fitLE3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")

```

In the third subdomian,we have a p value of 0.117>0.05, so we will keep all the questions.

## Fourth Subdomain
```{r,echo=F}
LE4model<-'LE4 =~LE4a_Confidence+LE4b_Confidence+LE4c_Confidence+LE4d_Confidence+LE4e_Confidence'
fitLE4 <- cfa(LE4model,data=PL.factor)
summary(fitLE4)
parameterEstimates(fitLE4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")

```

In the fourth subdomain, we have a p value of 0.153. We will keep all the questions in this subdomain.

## Fifth subdomain
```{r,echo=F}
LE5model<-'LE5 =~LE5a_Confidence+LE5b_Confidence+LE5c_Confidence+LE5d_Confidence'
fitLE5 <- cfa(LE5model,data=PL.factor)
summary(fitLE5)
parameterEstimates(fitLE5, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the fifth subdomain, we have a p-value of 0.123, so we will keep all the questions in this dubdomain.

## Sixth subdomain
```{r,echo=F}
LE6model<-'LE6 =~LE6a_Confidence+LE6b_Confidence+LE6c_Confidence+LE6d_Confidence'
fitLE6 <- cfa(LE6model,data=PL.factor)
summary(fitLE6)
parameterEstimates(fitLE6, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")


```

In the sixth subdomain, the p-value is 0.66>0.05. We will not drop any question in this subdomain.

# Conclusion / Discussion

For PER&FEEDBACK table, I dropped "PF1e_Confidence","PF2c_Confidence" and "PF5c_Confidence" so that P-value of all subdomains are greater than 0.05 finally.  
For the Learning Experience table, we will only drop "LE2a_Confidence" and keep all the remaining questions in order to let the P-value of all subdomains are greater than 0.05 finally.      
For the Learning Experience table, we will only drop "LT3c_Confidence" and keep all the remaining questions in order to let the P-value of all subdomains are greater than 0.05 finally.

## Appendix