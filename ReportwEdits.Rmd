---
title: "TELL Framework Survey Analysis Report"
author: "Tim Hogan, Xiaofan Xia, Yanwen Liu, Jingning Yang, Wan-Chi Hsin"
date: "12/04/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
library(lavaan)
library(magrittr)
library(tidyr)
library(knitr)
library(abind)
library(kableExtra)
survey <- read.csv("survey.csv")
```

# 1. Introduction
Our client, Catherine Ritz, a professor at Boston University’s Department of Education, administered a survey pilot, completed by 86 individuals. Her goal was to investigate how foreign language teachers felt about the TELL Framework, a set of suggested characteristics a model foreign language teachers should have. In particular, she was interested in seeing if they would differ by the teacher’s demographic or the language of teaching. Her survey included 18 questions regarding the teacher’s backgrounds, and 200 questions regarding the TELL Framework. In particular, she took the listed characteristics from four of the major domains, and asked two questions about each one: if the teacher thought it was important for model teaching, and if the teacher was confident in applying it.

At our intake meeting, our client discussed improving the survey design for her final study. In particular, she was looking for a way to reduce the number of survey questions. In this report, we will propose a method and structure to summarize and remove questions.

This report will first start with a description of the Data Structure, as well as our Data Analysis. We will then describe the methods we will use to analyze the data, followed by our analysis.

# 2. Data Structure and Expolatory Data Analysis

## TELL Framework Structure

The Teacher Effectiveness for Language Learning (TELL) framework is categorized into multiple domains. Each domain has its own set of individual characteristics, put into smaller groups. For the purpose of this report, we will call each of the large sets "domains", and each smaller group a "subdomain".

## Data Structure

We were provided the data in an excel file with 6 spreadsheets including one sheet of notes, one sheet of personal information, and four sheets of questions on the Teacher Effectiveness for Language Learning (TELL) framework. The dataset of personal information contains questions regarding respondents' teaching language and education background. 

Each sheet in the TELL Framework of the survey includes answers for part of one of four domains from the TELL Framework: planning, learning experience, learning tools, and performance & feedback. There are two questions asked for each characteristic, regarding the respondents' attitudes of contribution and Contributes towards the characteristic, with 200 questions in total. 

In this report, we will primarily focus on the questions regarding "Contributes". Additionally, we will refer to each question with its letter code, such as "PL1a". Each subdomain will be referred to by its shorter letter code, "PL1". 

## Exploratory Data Analysis

We conducted a basic Exploratory Data Analysis (EDA) for this project. Firstly, we focus on the time for respondents to complete this survey.

```{r hist of user review counts,fig.height=8,fig.width=7, warning = FALSE, message=FALSE, echo=FALSE}
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
survey <- read.csv("survey.csv")

#Uniform newest part time into POSIX objects:
survey$Start.Date <- strptime(survey$Start.Date, format = "%m/%d/%Y %H:%M:%S")
survey$End.Date <- strptime(survey$End.Date, format = "%m/%d/%Y %H:%M:%S")

#Time people used to finish the survey:
survey$time <- as.numeric(round(abs(difftime(survey$End.Date, survey$Start.Date, units = "mins")),2))

#Count number of NA values in rows:
survey$num <- apply(survey, MARGIN = 1, FUN=function(x) length(x[is.na(x)]))
time <- as.data.frame(na.omit(survey$time))
time$num <- survey$num[1:74]
colnames(time)[1] <- "Time people used(minutes)"
colnames(time)[2] <- "# of unanswered questions"
hist(time$`# of unanswered questions`,xlm = c(0,200), main = "Histogram of the frequency of unanswered questions in the survey", xlab="Number of unanswered questions") 
#Count number of NA values in rows:
survey$num <- apply(survey, MARGIN = 1, FUN=function(x) length(x[is.na(x)]))
time <- as.data.frame(na.omit(survey$time))
time$num <- survey$num[1:74]
colnames(time)[1] <- "Time people used(minutes)"
colnames(time)[2] <- "# of unanswered questions"

hist(time$`# of unanswered questions`,xlm = c(0,200), main = "Histogram of the frequency of unanswered questions in the survey", xlab="Number of unanswered questions") 
plot(density(time$`Time people used(minutes)`), main = "Density plot for timing", xlab = "Minutes people used in the survey")

```

From the histogram graph, it shows that most people (about 29 people) did not answer any questions in the survey, and the second high frequency (about 19 people) in the survey answered all questions, rest of people answered questions between 0 to 200. 

From the density plot, it shows most people spend no longer than about 100 minutes for this survey, and only few of people used about 600 minutes or ever longer than 1300 minutes.


## Data Cleaning

Data Cleaning was conducted using R, primarily using the tidyverse package. The sheets were read in and bound together by row, allowing each row to contain the background data and all of the answers of an individual. Extra answers attached to no questions were removed. Names were also changed to fit a consistent structure among questions, allowing them to be effectively analyzed.

## Concerns

Based on exploring the data, we found a few areas that may cause limitations. Firstly, many people did not answer most of the questions, meaning that the number of overall observations is limited. This may limit our analysis and our results.

# 3. Methods

We used a confirmatory factor analysis to assess how well questions can be grouped into their subdomains. A confirmatory factor analysis allows us to assess how well parts of a survey can fall within a proposed structure. Using this, we can try to group each survey questions into parts. If the questions all can be effectively grouped under a subdomain using a CFA, we can propose that each of those individual questions can be removed, and replaced with one question that addresses the listed subdomain. 
To do this, we used the lavaan package in R. A model was created for each subdomain, composed of all of its questions. We looked only at individuals who answered questions for each model, and excluded blank answers. For this report, we chose to focus on questions regarding Contributes. The questions regarding contribution fall outside of our scope, so we would recommend consulting a survey expert if you want to find a way to address those.

To construct a model, we followed the structure of the TELL Framework, as described in the "Data Strcture" part of this report. We then used a protocol to assess the model and reduce questions. First, we looked at the standard errors for each of the questions and the loading. If the values of each were too low (in each case, lower than ~0.55), the question was considered not a good fit in the subdomain, and removed from the model. The process then continued, and the p-value was collected afterwards. Then, we checked the p-value of the model. A p-value higher than 0.05 indicates that the questions are all similar under the model, and the grouping is good. If the p-value was lower, it means there was strong evidence that questions were not equal, and one of the questions could be removed. We continued this process until achieving a sufficient model, and then collected summary statistics.

There were a few additional cases we had to consider as well. For subdomains with three questions, rather than removing questions, a transformation was done in order to assess the subdomain. Additionally, subdomains with two questions cannot be analyzed using this method. Rather than treating them by themselves, they were grouped with another subdomain, effectively grouping them together. 

\newpage
# 4. Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
rowclean <- function(filename) {
  data <- read.csv(paste0(filename),na.string=c("","NA"))
  rownum <- max(which(is.na(data[2,])==F))
  clean <- data[,1:rownum]
  colnames(clean) = NULL
  names1 <- clean[1,1:5]
  names2 <- clean[2,6:rownum]
  names<- cbind(names1, names2)
  clean <- clean[3:nrow(clean),]
  colnames(clean) <- c(t(names))
  return(clean)
}
filenames <- list.files()
selectfiles <- filenames[grep("TELL Statements",filenames,perl=FALSE)]
resultlist <- vector("list",length(selectfiles))
for(i in 1:length(selectfiles)) {
  resultlist[[i]] <- rowclean(selectfiles[i])
}
aggregate <- abind(resultlist, along=2,force.array=F)
personinfo <- aggregate[,1:5]
surveyanswers <- aggregate[,-which(names(aggregate) %in% names(personinfo))]
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("LT1a _Contributes",colnames(surveyanswers)))] <- "LT1a_Contributes"
colnames(surveyanswers)[c(grep("LT5c_Contributes.1",colnames(surveyanswers)))] <- "LT56c_Contributes"
data.LT <- surveyanswers[,c(grep("LT",colnames(surveyanswers)))]
data.LT <- data.LT[,c(grep("Contributes",colnames(data.LT)))]
ans = c("I'm not sure what this means", "Strongly Disagree","Disagree","Neutral","Agree","Strongly Agree")
LT.ordered <- data.LT
LT.numer <- data.LT
for (i in 1:ncol(data.LT)) {
  LT.ordered[,i] <- factor(data.LT[,i],levels=ans)
  LT.numer[,i] <- as.numeric(LT.ordered[,i])
}
colnames(LT.numer) <- colnames(data.LT)
```


```{r, include=FALSE}
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("PL1a _Confidence",colnames(surveyanswers)))] <- "PL1a_Confidence"
colnames(surveyanswers)[c(grep("PL6c_Contributes.1",colnames(surveyanswers)))] <- "PL6c_Confidence"

data.PL <- surveyanswers[,c(grep("PL",colnames(surveyanswers)))]
data.PL <- data.PL[,c(grep("Contributes",colnames(data.PL)))]
ans = c("I'm not sure what this means","Strongly Disagree", "Disagree","Neutral","Agree","Strongly Agree")
PL.ordered <- data.PL
PL.numer <- data.PL
for (i in 1:ncol(data.PL)) {
  PL.ordered[,i] <- factor(data.PL[,i],levels=ans)
  PL.numer[,i] <- as.numeric(PL.ordered[,i])
}
colnames(PL.numer) <- colnames(data.PL)


#Lavaan Stuff

PL1 <- 'pl1 =~ PL1a_Contributes+PL1b_Contributes+PL1c_Contributes+PL1d_Contributes+PL1e_Contributes
PL1b_Contributes ~~ PL1d_Contributes'
fit1 <- cfa(PL1,data=PL.numer,std.lv=T) 
PL1table <- c("PL1","PL1a,PL1b,PL1c,PL1d,PL1e",0.220, round(fitMeasures(fit1)[c("cfi")],digits=3),round(fitMeasures(fit1)[c("tli")],digits=3))


PL2 <- 'pl2 =~ aa*PL2a_Contributes + PL2b_Contributes+aa*PL2c_Contributes'
fit2 <- cfa(PL2,data=PL.numer,std.lv=T) #EStimate Error Variances
PL2table <- c("PL2","PL2a,PL2b,PL2c",0.601, round(fitMeasures(fit2)[c("cfi")],digits=3),round(fitMeasures(fit2)[c("tli")],digits=3))

PL3 <- 'pl3 =~ PL3a_Contributes + PL3b_Contributes + PL3d_Contributes + PL3e_Contributes
PL3b_Contributes ~~ PL3d_Contributes'
fit3 <- cfa(PL3,data=PL.numer,std.lv=T) #EStimate Error Variances
PL3table <- c("PL3","PL3a,PL3b,PL3d,PL3e",0.250, round(fitMeasures(fit3)[c("cfi")],digits=3),round(fitMeasures(fit3)[c("tli")],digits=3))


PL4 <- 'pl4 =~ PL4a_Contributes+aa*PL4b_Contributes+aa*PL4c_Contributes'
fit4 <- cfa(PL4,data=PL.numer,std.lv=T) #EStimate Error Variances
PL4table <- c("PL4","PL4a,PL4b,PL4c",0.599, round(fitMeasures(fit4)[c("cfi")],digits=3),round(fitMeasures(fit4)[c("tli")],digits=3))

PL5 <- 'pl5 =~ PL5a_Contributes + PL5b_Contributes + PL5c_Contributes + PL5d_Contributes'
fit5 <- cfa(PL5,data=PL.numer,std.lv=T) #EStimate Error Variances
PL5table <- c("PL5","PL5a,PL5b,PL5c,PL5d",0.121, round(fitMeasures(fit5)[c("cfi")],digits=3),round(fitMeasures(fit5)[c("tli")],digits=3))


PL6 <- 'pl6 =~ PL6a_Contributes + aa*PL6b_Contributes + aa*PL6d_Contributes'
fit6 <- cfa(PL6,data=PL.numer,std.lv=T) #EStimate Error Variances
PL6table <- c("PL6","PL6a,PL6b,PL6d",0.850, round(fitMeasures(fit6)[c("cfi")],digits=3),round(fitMeasures(fit6)[c("tli")],digits=3))

PL7 <- 'pl7 =~ aa*PL7a_Contributes + aa*PL7b_Contributes + PL7c_Contributes'
fit7 <- cfa(PL7,data=PL.numer,std.lv=T) #EStimate Error Variances
PL7table <- c("PL7","PL7a,PL7b,PL7c",0.337, round(fitMeasures(fit7)[c("cfi")],digits=3),round(fitMeasures(fit7)[c("tli")],digits=3))

PL8 <- 'pl8 =~ aa*PL8b_Contributes + aa*PL8c_Contributes + PL8d_Contributes'
fit8 <- cfa(PL8,data=PL.numer,std.lv=T) #EStimate Error Variances
PL8table <- c("PL8","PL8b,PL8c,PL8d",0.186, round(fitMeasures(fit8)[c("cfi")],digits=3),round(fitMeasures(fit8)[c("tli")],digits=3))
```

## Planning Domain
```{r,echo=F}
PLTable2 <- rbind(PL1table,PL2table,PL3table,PL4table,PL5table,PL6table,PL7table,PL8table)
colnames(PLTable2) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(PLTable2) <- NULL
kable(PLTable2,digits=3,format="pandoc",caption="'Planning' Contributes Subdomain Summary")
```

Summary statistics for the subdomains of PL1 are shown in Table 1. Questions were removed based on our protocol, and the remaining questions are shown in the “Questions” table. Questions PL1f,PL1g, PL3c, PL6c, and PL8a were removed. These questions were found not to fit in these groupings, and would need to be treated separately when removing questions.
The models meet the gold standard of a Comparative Fit Index (CFI) of 0.90, indicating that there is not a major discrepancy between the hypothetical models and the data. The Tucker-Lewis Index (TLI) for each model are also close or lower to 1, supporting that the data and models seem to be close. 
The P-values for each of the model all are relatively high, indicating that they most likely follow the null hypothesis. Effectively, this means that the questions within the model can be grouped into their subdomain. 


## Learning Tool Domain

```{r, include=F}
data.LT <- surveyanswers[,c(grep("LT",colnames(surveyanswers)))]
data.LT <- data.LT[,c(grep("Contributes",colnames(data.LT)))]
ans = c("I'm not sure what this means", "Strongly Disagree","Disagree","Neutral","Agree","Strongly Agree")
LT.ordered <- data.LT
LT.numer <- data.LT
for (i in 1:ncol(data.LT)) {
  LT.ordered[,i] <- factor(data.LT[,i],levels=ans)
  LT.numer[,i] <- as.numeric(LT.ordered[,i])
}
colnames(LT.numer) <- colnames(data.LT)

LT1 <- 'lt1 =~ aa*LT1a_Contributes + aa*LT1b_Contributes + LT1c_Contributes '
fit1 <- cfa(LT1,data=LT.numer,std.lv=T)
LT1table <- c("LT1","LT1a,LT1b,LT1c",0.885, round(fitMeasures(fit1)[c("cfi")],digits=3),round(fitMeasures(fit1)[c("tli")],digits=3))

LT2new <- 'lt2new =~ LT1a_Contributes + LT1b_Contributes + LT1c_Contributes+LT2a_Contributes'
fit2new <- cfa(LT2new,data=LT.numer,std.lv=T)
LT2table <- c("LT1&LT2","LT1a,LT1b,LT1c,LT2a",0.659, round(fitMeasures(fit2new)[c("cfi")],digits=3),round(fitMeasures(fit2new)[c("tli")],digits=3))


LT3new <- 'lt3 =~ LT3a_Contributes+LT3b_Contributes+LT3c_Contributes+LT3d_Contributes+LT4b_Contributes'
fit3new <- cfa(LT3new,data=LT.numer,std.lv=T)
LT3table <- c("LT3&LT4","LT3a,LT3b,LT3c,LT3d,LT4b",0.149, round(fitMeasures(fit3new)[c("cfi")],digits=3),round(fitMeasures(fit3new)[c("tli")],digits=3))


LT5 <- 'lt5 =~ aa*LT5a_Contributes + LT5b_Contributes + aa*LT5c_Contributes'
fit5 <- cfa(LT5,data=LT.numer,std.lv=T)
LT5table <- c("LT5","LT5a,LT5b,LT5c",0.141, round(fitMeasures(fit5)[c("cfi")],digits=3),round(fitMeasures(fit5)[c("tli")],digits=3))
```

```{r,echo=F}
LTTable <- rbind(LT2table,LT3table,LT5table)
colnames(LTTable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(LTTable) <- NULL
kable(LTTable,digits=3,format="pandoc",caption="'Learning Tools' Subdomain Summary")
```

The summary statistics for the Learning Tools subdomains are shown in Table 2. As before, questions included in each subdomain are listed in the "Questions" column. The questions removed due to the protocal were LT2b, LT2c, LT4a, and LT4c, which may need to be treated separately.  The CFI and TLI both seem high and close to 1 respectively, showing that the data and proposed models are relatively close. LT1 may need to be considered more closely, since its LT1 is relatively larger than the rest of these values. However, it still seems to show a relatively close comparison between the data and proposed models.
Once again, our p-values indicate that the null hypothesis cannot be rejected, and the questions can effectively be grouped into a subdomain.

## Per & Feedback Domain

```{r,echo=FALSE}
# First subdomain
data.PF <- surveyanswers[,c(grep("PF",colnames(surveyanswers)))]
data.PF <- data.PF[,c(grep("Contributes",colnames(data.PF)))]
ans = c("I'm not sure what this means", "Strongly Disagree","Disagree","Neutral","Agree","Strongly Agree")
PF.ordered <- data.PF
PF.number <- data.PF
for (i in 1:ncol(data.PF)) {
  PF.ordered[,i] <- factor(data.PF[,i],levels=ans)
  PF.number[,i] <- as.numeric(PF.ordered[,i])
}
colnames(PF.number) <- colnames(data.PF)


m1 <- 'PF1 =~ PF1a_Contributes + PF1b_Contributes + PF1c_Contributes + PF1d_Contributes + PF1e_Contributes'
fit1 <- cfa(m1,data = PF.number, std.lv=TRUE)
PF1table <- c("PF1","PF1a,PF1b,PF1c,PF1d,PF1e",0.707, round(fitMeasures(fit1)[c("cfi")],digits=3),round(fitMeasures(fit1)[c("tli")],digits=3))


# Second subdomain
m2.1 <- 'PF2 =~ PF2a_Contributes + PF2b_Contributes + PF2c_Contributes + PF2d_Contributes'
fit2.1 <- cfa(m2.1,data = PF.number, std.lv=TRUE)
PF2table <- c("PF2","PF2a,PF2b,PF2c,PF2d",0.575, round(fitMeasures(fit2.1)[c("cfi")],digits=3),round(fitMeasures(fit2.1)[c("tli")],digits=3))


# Third subdomain
m3.2 <- 'PF3 =~ PF3a_Contributes + PF3c_Contributes + PF3e_Contributes'
fit3.2 <- cfa(m3.2,data = PF.number, std.lv=TRUE)
PF3table <- c("PF3","PF3a,PF3c,PF3e",0.000, round(fitMeasures(fit3)[c("cfi")],digits=3),round(fitMeasures(fit3)[c("tli")],digits=3))



# Fourth subdomain
m4 <- 'PF4 =~ PF4a_Contributes + PF4b_Contributes
       PF5 =~ PF5a_Contributes + PF5b_Contributes + PF5c_Contributes'
fit4 <- cfa(m4,data = PF.number, std.lv=TRUE)

m4.1 <- 'PF4 =~ PF4a_Contributes + PF4b_Contributes
         PF5 =~ PF5a_Contributes + PF5b_Contributes'
fit4.1 <- cfa(m4.1,data = PF.number, std.lv=TRUE)
PF4table <- c("PF4 & PF5","PF4a,PF4b,PF5a,PF5b",0.146, round(fitMeasures(fit4.1)[c("cfi")],digits=3),round(fitMeasures(fit4.1)[c("tli")],digits=3))
```

```{r,echo=F}
PFTable <- rbind(PF1table,PF2table,PF3table,PF4table)
colnames(PFTable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(PFTable) <- NULL
kable(PFTable,digits=3,format="pandoc",caption="'Performance & Feedback' Subdomain Summary")

```

The summary statistics for the Performance and Feedback subdomain is shown in Table 3. Excluded questions from our protocol were PF1e, PF2c, and PF5c. Since PF4 only contained two questions, following our protocol, it was treated in combination with PL5 in order to be assessed with our CFA method. 

Once again, the calculated CFI and TLI are above 0.9 and close to 1 respectively, indicating that the data and proposed models follow each other well. Additionally, p-values are higher than the 0.05 threshold, indicating that these subdomains can be used to group questions together effectively.

## Learning Experience Domain

```{r,echo=FALSE}
PL.factor <- select(survey,'LE1a_Contributes':'LE6d_Contributes')
PL.factor <-lapply(PL.factor,function(x) as.numeric(as.factor(x)))
PL.factor <- as.data.frame(PL.factor)

LE1model<-'LE1 =~LE1a_Contributes+LE1b_Contributes+LE1c_Contributes+LE2e_Contributes'
fitLE1 <- cfa(LE1model,data=PL.factor,std.lv=T)
LE1table <- c("LE1","LE1a,LE1b,LE1c,LE1e",0.52, round(fitMeasures(fitLE1)[c("cfi")],digits=3),round(fitMeasures(fitLE1)[c("tli")],digits=3))

LE2model<-'LE2 =~LE2a_Contributes+LE2c_Contributes+LE2d_Contributes+LE2e_Contributes+LE2f_Contributes'
fitLE2 <- cfa(LE2model,data=PL.factor,std.lv=T)
LE2table <- c("LE2","LE2a,LE2c,LE2d,LE2e,LE2f",0.176, round(fitMeasures(fitLE2)[c("cfi")],digits=3),round(fitMeasures(fitLE2)[c("tli")],digits=3))


LE3model<-'LE3 =~LE3b_Contributes+LE3d_Contributes+LE3e_Contributes+LE3f_Contributes+LE3g_Contributes'
fitLE3 <- cfa(LE3model,data=PL.factor,std.lv=T)
LE3table <- c("LE3","LE3b,LE3d,LE3e,LE3f,LE3g",0.142, round(fitMeasures(fitLE3)[c("cfi")],digits=3),round(fitMeasures(fitLE3)[c("tli")],digits=3))


LE4model<-'LE4 =~LE4a_Contributes+LE4b_Contributes+LE4c_Contributes+LE4e_Contributes'
fitLE4 <- cfa(LE4model,data=PL.factor,std.lv=T)
LE4table <- c("LE4","LE4a,LE4b,LE4c,LE4e",0.811, round(fitMeasures(fitLE4)[c("cfi")],digits=3),round(fitMeasures(fitLE4)[c("tli")],digits=3))


LE5model<-'LE5 =~LE5a_Contributes+LE5b_Contributes+LE5c_Contributes+LE5d_Contributes'
fitLE5 <- cfa(LE5model,data=PL.factor,std.lv=T)
LE5table <- c("LE5","LE5a,LE5b,LE5c,LE5d",0.765, round(fitMeasures(fitLE5)[c("cfi")],digits=3),round(fitMeasures(fitLE5)[c("tli")],digits=3))


LE6model<-'LE6 =~aa*LE6a_Contributes+LE6b_Contributes+aa*LE6d_Contributes'
fitLE6 <- cfa(LE6model,data=PL.factor,std.lv=T)
LE6table <- c("LE6","LE6a,LE6b,LE6d",0.33, round(fitMeasures(fitLE6)[c("cfi")],digits=3),round(fitMeasures(fitLE6)[c("tli")],digits=3))
```

```{r,echo=F}
LETable <- rbind(LE1table,LE2table,LE3table,LE4table,LE5table,LE6table)
colnames(LETable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(LETable) <- NULL
kable(LETable,digits=3,format="pandoc",caption="'Learning Experience' Subdomain Summary")

```

The results for the Learning Experience Domain can be shown in Table 4. The questions removed due to the question removal protocol are LE1d, LE2b, LE3a, LE3c, LE4d, and LE6c. These questions may need to be treated separately when restructuring the survey.

Our CFI and TLI values are both high and close to 1, indicating that the models fit the data. The TLI for LE4 is relatively higher than the rest, which may mean it needs to be considered separately. However, it is still relatively close to 1, and still indicates a decent fit between data and model. The p-values are above our threshold of 0.05, indicating that each one groups each set of questions well.

# 5. Conclusion

In this report, we have proposed a structure to group and remove large set of question based on the structure of the TELL Framework. In our analysis, we used a Confirmatory Factor Analysis to show that many of the survey questions can be grouped in a larger structure. This may highlight a method to reduce question number, where, rather than asking each of the questions, one question is asked for each group. However, this will require a change in questioning and possibly a change in structure.

The questions removed from the subdomains must be considered separately. Usually, they were removed because the way they were answered followed a significantly different pattern from other questions. There may be a final structure that does group these with the rest. Our analysis only shows that they don't fit best under the groupings provided by the TELL framework.

## Appendix

## Planning Domain Analysis

## First Subdomain

```{r,echo=F}
PL1 <- 'pl1 =~ PL1a_Contributes+PL1b_Contributes+PL1c_Contributes+PL1d_Contributes+PL1e_Contributes
PL1b_Contributes ~~ PL1d_Contributes'
fit1 <- cfa(PL1,data=PL.numer,std.lv=T) 
summary(fit1,standardized=T)
parameterEstimates(fit1,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Second Subdomain

```{r,echo=F}
PL2 <- 'pl2 =~ aa*PL2a_Contributes + PL2b_Contributes+aa*PL2c_Contributes'
fit2 <- cfa(PL2,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit2,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit2,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Third Subdomain

```{r,echo=F}
PL3 <- 'pl3 =~ PL3a_Contributes + PL3b_Contributes + PL3d_Contributes + PL3e_Contributes
PL3b_Contributes ~~ PL3d_Contributes'
fit3 <- cfa(PL3,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit3,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit3,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Fourth Subdomain

```{r,echo=F}
PL4 <- 'pl4 =~ PL4a_Contributes+aa*PL4b_Contributes+aa*PL4c_Contributes'
fit4 <- cfa(PL4,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit4,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)#No degrees of freedom if there are three questions.
parameterEstimates(fit4,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Fifth Subdomain

```{r,echo=F}
PL5 <- 'pl5 =~ PL5a_Contributes + PL5b_Contributes + PL5c_Contributes + PL5d_Contributes'
fit5 <- cfa(PL5,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit5,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit5,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Sixth Subdomain

```{r,echo=F}
PL6 <- 'pl6 =~ PL6a_Contributes + aa*PL6b_Contributes + aa*PL6d_Contributes'
fit6 <- cfa(PL6,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit6,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit6,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Seventh Subdomain

```{r,echo=F}
PL7 <- 'pl7 =~ aa*PL7a_Contributes + aa*PL7b_Contributes + PL7c_Contributes'
fit7 <- cfa(PL7,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit7,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit7,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

## Eighth Subdomain

```{r,echo=F}
PL8 <- 'pl8 =~ aa*PL8b_Contributes + aa*PL8c_Contributes + PL8d_Contributes'
fit8 <- cfa(PL8,data=PL.numer,std.lv=T) #EStimate Error Variances
summary(fit8,standardized=T) # 6 parameters for each model. In each thing there would be 3 manifest items (6 unique pieces)
parameterEstimates(fit8,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```



## Learning Tool Domain Analysis
For Learning Tools table in TELL Statements, we numeric character answers of LT 1a~5c Contributes, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LT1 has 3 variables: LT1a_Contributes, LT1b_Contributes, LT1c_Contributes).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results:

# First subdomain:

```{r echo=FALSE, message=FALSE, warning=FALSE}
LT1 <- 'lt1 =~ aa*LT1a_Contributes + aa*LT1b_Contributes + LT1c_Contributes '
fit1 <- cfa(LT1,data=LT.numer,std.lv=T)
summary(fit1,standardized=T)
parameterEstimates(fit1,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.885>0.05. Factor loadings: a-0.825;b-0.904;c-0.711. cfi=1;ltli=1.063.
```

Since p-value of the first subdomain is 0.885> 0.05 and all factor loadings are larger than 0.55, there is no need to make any change in the first subdomain. 

# Second subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT2 <- 'lt2 =~ LT2a_Contributes + aa*LT2b_Contributes + aa*LT2c_Contributes'
fit2 <- cfa(LT2,data=LT.numer,std.lv=T)
summary(fit2,standardized=T)
parameterEstimates(fit2,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.077>0.05. Factor loadings: a-1.124;b-0.477;c-0.448. cfi=0.901; tli=0.703.
```

Since p-value of the second subdomain is 0.077 > 0.05 and question b & c have factor loadings smaller than 0.55, so we suggest that these two questions do not fit in this subdomain. 

# First subdomain and second subdomain

```{r echo=FALSE, message=FALSE, warning=FALSE }
LT2new <- 'lt2new =~ LT1a_Contributes + LT1b_Contributes + LT1c_Contributes+ LT2a_Contributes'
fit2new <- cfa(LT2new,data=LT.numer,std.lv=T)
summary(fit2new,standardized=T)
parameterEstimates(fit2new,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.659>0.05. Factor loadings: 1a-0.799;1b-0.944;1c-0.676; 2a-0.899;cfi=1; tli=1.038.
```

After adding the remain question LT2a into LT1 subdomain, the model fits well. 

# Third subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT3 <- 'lt3 =~ LT3a_Contributes + LT3b_Contributes + LT3c_Contributes + LT3d_Contributes'
fit3 <- cfa(LT3,data=LT.numer,std.lv=T)
summary(fit3,standardized=T)
parameterEstimates(fit3,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#p-value-0.083;a-0.891;b-0.946;c-0.819;d-0.737;cfi-0.972;tli-0.916.
```

Since p-value of the third subdomain is 0.083 < 0.05, and all factor loadings are larger than 0.55, all questions fit well in the subdomain. 


# Fourth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT4 <- 'lt4 =~ aa*LT4a_Contributes + LT4b_Contributes + aa*LT4c_Contributes'
fit4 <- cfa(LT4,data=LT.numer,std.lv=T)
summary(fit4,standardized=T)
parameterEstimates(fit4,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.038. Factor loadings: a-0.446;b-1.27;c-0.481. cfi=0.895; tli=0.686.
```

Since p-value of the fourth subdomain is 0.038 < 0.05, we need to remove the  (a&c) with small factor loadings to fit the model.

# Third subdomain and fourth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT4new <- 'lt3 =~ LT3a_Contributes + LT3b_Contributes + LT3c_Contributes + LT3d_Contributes+ LT4b_Contributes'
fit4new <- cfa(LT4new,data=LT.numer,std.lv=T)
summary(fit4new,standardized=T)
parameterEstimates(fit4new,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#p-value-0.149;3a-0.895;3b-0.929;3c-0.834;3d-0.746;4b-0.794;cfi-0.977;tli-0.955.
```

After combining the LT3 subdomain and LT4b, the model fits very well.

# Fifth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT5 <- 'lt5 =~ aa*LT5a_Contributes + LT5b_Contributes + aa*LT5c_Contributes'
fit5 <- cfa(LT5,data=LT.numer,std.lv=T)
summary(fit5,standardized=T)
parameterEstimates(fit5,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.141. Factor loadings: a-0.521;b-0.99;c-0.556. cfi=0.946; tli=0.837.
```

Since p-value of the fifth subdomain is 0.141 > 0.05 and all factor loadings are larger than or around 0.55, there is no need to make any change in the fifth subdomain.

## PER & FEEDBACK Domain Analysis
For PER&FEEDBACK table in TELL Statements, I numeric character answers of PF 1a~5c Contributes, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain whose variables should greater than 2 (ex: PF1 has 5 variables: PF1a_Contributes, PF1b_Contributes, PF1c_Contributes,PF1d_Contributes and PF1e_Contributes), or the P-value of that model will become NA. And we get an exception in PF table: PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we have an available P-value. Second, we find factor loadings of each variables in each subdomain and record them. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again.
Following are detailed results   

# First subdomain:
```{r, echo=FALSE}
colnames(surveyanswers)[c(grep("PF1a _Contributes",colnames(surveyanswers)))] <- "PF1a_Contributes"
colnames(surveyanswers)[c(grep("PF5c_Contributes.1",colnames(surveyanswers)))] <- "PF5c_Contributes"
data.PF <- surveyanswers[,c(grep("PF",colnames(surveyanswers)))]
data.PF <- data.PF[,c(grep("Contributes",colnames(data.PF)))]
ans = c("I'm not sure what this means", "Strongly Disagree","Disagree","Neutral","Agree","Strongly Agree")
PF.ordered <- data.PF
PF.number <- data.PF
for (i in 1:ncol(data.PF)) {
  PF.ordered[,i] <- factor(data.PF[,i],levels=ans)
  PF.number[,i] <- as.numeric(PF.ordered[,i])
}
colnames(PF.number) <- colnames(data.PF)
# First subdomain
m1 <- 'PF1 =~ PF1a_Contributes + PF1b_Contributes + PF1c_Contributes + PF1d_Contributes + PF1e_Contributes'
fit1 <- cfa(m1,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit1,standardized=T)
#P-value = 0.707
parameterEstimates(fit1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.591; 0.839; 0.96; 0.728; 0.941
```

Since p-value of first subdomain is 0.707 > 0.05, thus no futher work to do.

# Second subdomain:
```{r, echo=FALSE}
# Second subdomain
m2 <- 'PF2 =~ PF2a_Contributes + PF2b_Contributes + PF2c_Contributes + PF2d_Contributes + PF2e_Contributes'
fit2 <- cfa(m2,data = PF.number, std.lv=TRUE)
summary(fit2,standardized=T)
#P-value = 0.000
parameterEstimates(fit2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.923; 0.83; 0.922; 0.713; 0.559
```

Since p-value of first subdomain is 0.000 < 0.05, and the factor loadings of "PF2e_Contributes" is lowest, thus, we try to drop it from the second subdomain:
```{r, echo=FALSE}
m2.1 <- 'PF2 =~ PF2a_Contributes + PF2b_Contributes + PF2c_Contributes + PF2d_Contributes'
fit2.1 <- cfa(m2.1,data = PF.number, std.lv=TRUE)
summary(fit2.1,standardized=T)
```
P-value = 0.575 > 0.05, thus we can stay here for the second subdomain.


# Third subdomain:
```{r, echo=FALSE}
m3 <- 'PF3 =~ PF3a_Contributes + PF3b_Contributes + PF3c_Contributes + PF3d_Contributes + PF3e_Contributes'
fit3 <- cfa(m3,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit3,standardized=T)
#P-value = 0.000
parameterEstimates(fit3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.819; 0.6; 0.881; 0.543; 0.923
```

Since p-value < 0.05, and the factor loadings of "PF3d_Contributes" is lowest, thus, we try to drop it from the second subdomain:
```{r}
m3.1 <- 'PF3 =~ PF3a_Contributes + PF3b_Contributes + PF3c_Contributes + PF3e_Contributes'
fit3.1 <- cfa(m3.1,data = PF.number, std.lv=TRUE)
summary(fit3.1,standardized=T)
#p value = 0 
parameterEstimates(fit3.1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

```{r}
m3.2 <- 'PF3 =~ PF3a_Contributes + PF3c_Contributes + PF3e_Contributes'
fit3.2 <- cfa(m3.2,data = PF.number, std.lv=TRUE)
# lavaan WARNING: some estimated ov variances are negative ???????????????????
summary(fit3.2,standardized=T)
#p value = 0 
parameterEstimates(fit3.2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```
??????????

# Fourth subdomain:
PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we can get an available P-value.
```{r, echo=FALSE}
m4 <- 'PF4 =~ PF4a_Contributes + PF4b_Contributes
       PF5 =~ PF5a_Contributes + PF5b_Contributes + PF5c_Contributes'
fit4 <- cfa(m4,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit4,standardized=T)
#P-value = 0.000
parameterEstimates(fit4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.839; 0.987; 0.706; 1.014; 0.67
```

Since P-value is 0.000 < 0.05, and the factor loadings of "PF5c_Contributes" is lowest, thus, we try to drop it from the fourth subdomain:
```{r, echo=FALSE}
m4.1 <- 'PF4 =~ PF4a_Contributes + PF4b_Contributes
       PF5 =~ PF5a_Contributes + PF5b_Contributes'
fit4.1 <- cfa(m4.1,data = PF.number, std.lv=TRUE)
summary(fit4.1,standardized=T)
```
P-value is 0.146 > 0.05, thus no longer remodel this subdomain.

## Learning Experience Domain Analysis
For learning experience table in TELL Statements, we numeric character answers of LE 1a~6d Contributes, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LE1 has 5 variables: LE1a_Contributes, LE1b_Contributes, LE1c_Contributes,LE1d_Contributes and LE1e_Contributes).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results

## First subdomian
```{r,echo=F}
PL.factor <- select(survey,'LE1a_Contributes':'LE6d_Contributes')
PL.factor <-lapply(PL.factor,function(x) as.numeric(as.factor(x)))
PL.factor <- as.data.frame(PL.factor)
LE1model<-'LE1 =~LE1a_Contributes+LE1b_Contributes+LE1c_Contributes+LE1d_Contributes+LE1e_Contributes'
fitLE1 <- cfa(LE1model,data=PL.factor)
summary(fitLE1)
parameterEstimates(fitLE1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
LE1model<-'LE1 =~LE1a_Contributes+LE1b_Contributes+LE1c_Contributes+LE1e_Contributes'
fitLE1 <- cfa(LE1model,data=PL.factor)
summary(fitLE1)
parameterEstimates(fitLE1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

The factor loading of LE1d is 0.503, so we will remove it. Then the new model has 0.446 p value.

## Second Subdomain
```{r,echo=F}
LE2model<-'LE2 =~LE2a_Contributes+LE2b_Contributes+LE2c_Contributes+LE2d_Contributes+LE2e_Contributes+LE2f_Contributes'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")

```

In the second subdomain, the loading of LE2b is less than 0.55, so we will drop it and keep all the others.
```{r,echo=F}
LE2model<-'LE2 =~LE2a_Contributes+LE2c_Contributes+LE2d_Contributes+LE2e_Contributes+LE2f_Contributes'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```


## Third Subdomain
```{r,echo=F}
LE3model<-'LE3 =~LE3b_Contributes+LE3d_Contributes+LE3e_Contributes+LE3f_Contributes+LE3g_Contributes'
fitLE3 <- cfa(LE3model,data=PL.factor)
summary(fitLE3)
parameterEstimates(fitLE3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the third subdomian,we drop the LE3a, LE3c due to low factor loading and keeps all the others.

## Fourth Subdomain
```{r,echo=F}
LE4model<-'LE4 =~LE4a_Contributes+LE4b_Contributes+LE4c_Contributes+LE4e_Contributes'
fitLE4 <- cfa(LE4model,data=PL.factor)
summary(fitLE4)
parameterEstimates(fitLE4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the fourth subdomain, we drop LE4d due to low loading.

## Fifth subdomain
```{r,echo=F}
LE5model<-'LE5 =~LE5a_Contributes+LE5b_Contributes+LE5c_Contributes+LE5d_Contributes'
fitLE5 <- cfa(LE5model,data=PL.factor)
summary(fitLE5)
parameterEstimates(fitLE5, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the fifth subdomain, we have a p-value of 0.765, so we will keep all the questions in this dubdomain.

## Sixth subdomain
```{r,echo=F}
LE6model<-'LE6 =~aa*LE6b_Contributes+LE6c_Contributes+aa*LE6d_Contributes'
fitLE6 <- cfa(LE6model,data=PL.factor)
summary(fitLE6)
parameterEstimates(fitLE6, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the sixth subdomain, the a is dropping due to low factor loading.

