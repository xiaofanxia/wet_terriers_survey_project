---
title: "TELL Framework Survey Analysis Report"
author: "Tim Hogan, Xiaofan Xia, Yanwen Liu, Jingning Yang, Wan-Chi Hsin"
date: "12/04/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
library(lavaan)
library(magrittr)
library(tidyr)
library(knitr)
library(abind)
library(kableExtra)
survey <- read.csv("survey.csv")
```

# 1. Introduction
Our client, Catherine Ritz, a professor at Boston University’s Department of Education, administered a survey pilot, completed by 86 individuals. Her goal was to investigate how foreign language teachers felt about the TELL Framework, a set of suggested characteristics a model foreign language teachers should have. In particular, she was interested in seeing if they would differ by the teacher’s demographic or the language of teaching. Her survey included 18 questions regarding the teacher’s backgrounds, and 200 questions regarding the TELL Framework. In particular, she took the listed characteristics from four of the major domains, and asked two questions about each one: if the teacher thought it was important for model teaching, and if the teacher was confident in applying it.

At our intake meeting, our client discussed improving the survey design for her final study. In particular, she was looking for a way to reduce the number of survey questions. In this report, we will propose a method and structure to summarize and remove questions.

This report will first start with a description of the Data Structure, as well as our Data Analysis. We will then describe the methods we will use to analyze the data, followed by our analysis.

# 2. Data Structure and Expolatory Data Analysis

## TELL Framework Structure

The Teacher Effectiveness for Language Learning (TELL) framework is categorized into multiple domains. Each domain has its own set of individual characteristics, put into smaller groups. For the purpose of this report, we will call each of the large sets "domains", and each smaller group a "subdomain".

## Data Structure

We were provided the data in an excel file with 6 spreadsheets including one sheet of notes, one sheet of personal information, and four sheets of questions on the Teacher Effectiveness for Language Learning (TELL) framework. The dataset of personal information contains questions regarding respondents' teaching language and education background. 

Each sheet in the TELL Framework of the survey includes answers for part of one of four domains from the TELL Framework: planning, learning experience, learning tools, and performance & feedback. There are two questions asked for each characteristic, regarding the respondents' attitudes of contribution and confidence towards the characteristic, with 200 questions in total. 

In this report, we will primarily focus on the questions regarding "Confidence". Additionally, we will refer to each question with its letter code, such as "PL1a". Each subdomain will be referred to by its shorter letter code, "PL1". 

## Exploratory Data Analysis

We conducted a basic Exploratory Data Analysis (EDA) for this project. Firstly, we focus on the time for respondents to complete this survey.

```{r hist of user review counts,fig.height=8,fig.width=7, warning = FALSE, message=FALSE, echo=FALSE}
library(lubridate)
library(hms)
library(ggplot2)
library(dplyr)
library(cowplot)
survey <- read.csv("survey.csv")

#Uniform newest part time into POSIX objects:
survey$Start.Date <- strptime(survey$Start.Date, format = "%m/%d/%Y %H:%M:%S")
survey$End.Date <- strptime(survey$End.Date, format = "%m/%d/%Y %H:%M:%S")

#Time people used to finish the survey:
survey$time <- as.numeric(round(abs(difftime(survey$End.Date, survey$Start.Date, units = "mins")),2))

#Count number of NA values in rows:
survey$num <- apply(survey, MARGIN = 1, FUN=function(x) length(x[is.na(x)]))
time <- as.data.frame(na.omit(survey$time))
time$num <- survey$num[1:74]
colnames(time)[1] <- "Time people used(minutes)"
colnames(time)[2] <- "# of unanswered questions"
hist(time$`# of unanswered questions`,xlm = c(0,200), main = "Histogram of the frequency of unanswered questions in the survey", xlab="Number of unanswered questions") 

```

From this graph, it shows that most people (about 29 people) did not answer any questions in the survey, and the second high frequency (about 19 people) in the survey answered all questions, rest of people answered questions between 0 to 200. 


## Data Cleaning

Data Cleaning was conducted using R, primarily using the tidyverse package. The sheets were read in and bound together by row, allowing each row to contain the background data and all of the answers of an individual. Extra answers attached to no questions were removed. Names were also changed to fit a consistent structure among questions, allowing them to be effectively analyzed.

## Concerns

Based on exploring the data, we found a few areas that may cause limitations. Firstly, many people did not answer most of the questions, meaning that the number of overall observations is limited. This may limit our analysis and our results.

# 3. Methods

We used a confirmatory factor analysis to assess how well questions can be grouped into their subdomains. A confirmatory factor analysis allows us to assess how well parts of a survey can fall within a proposed structure. Using this, we can try to group each survey questions into parts. If the questions all can be effectively grouped under a subdomain using a CFA, we can propose that each of those individual questions can be removed, and replaced with one question that addresses the listed subdomain. 
To do this, we used the lavaan package in R. A model was created for each subdomain, composed of all of its questions. We looked only at individuals who answered questions for each model, and excluded blank answers. For this report, we chose to focus on questions regarding confidence. The questions regarding contribution fall outside of our scope, so we would recommend consulting a survey expert if you want to find a way to address those.

To construct a model, we followed the structure of the TELL Framework, as described in the "Data Strcture" part of this report. We then used a protocol to assess the model and reduce questions. First, we looked at the standard errors for each of the questions and the loading. If the values of each were too low (in each case, lower than ~0.55), the question was considered not a good fit in the subdomain, and removed from the model. The process then continued, and the p-value was collected afterwards. Then, we checked the p-value of the model. A p-value higher than 0.05 indicates that the questions are all similar under the model, and the grouping is good. If the p-value was lower, it means there was strong evidence that questions were not equal, and one of the questions could be removed. We continued this process until achieving a sufficient model, and then collected summary statistics.

There were a few additional cases we had to consider as well. For subdomains with three questions, rather than removing questions, a transformation was done in order to assess the subdomain. Additionally, subdomains with two questions cannot be analyzed using this method. Rather than treating them by themselves, they were grouped with another subdomain, effectively grouping them together. 

\newpage
# 4. Analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
rowclean <- function(filename) {
  data <- read.csv(paste0(filename),na.string=c("","NA"))
  rownum <- max(which(is.na(data[2,])==F))
  clean <- data[,1:rownum]
  colnames(clean) = NULL
  names1 <- clean[1,1:5]
  names2 <- clean[2,6:rownum]
  names<- cbind(names1, names2)
  clean <- clean[3:nrow(clean),]
  colnames(clean) <- c(t(names))
  return(clean)
}
filenames <- list.files()
selectfiles <- filenames[grep("TELL Statements",filenames,perl=FALSE)]
resultlist <- vector("list",length(selectfiles))
for(i in 1:length(selectfiles)) {
  resultlist[[i]] <- rowclean(selectfiles[i])
}
aggregate <- abind(resultlist, along=2,force.array=F)
personinfo <- aggregate[,1:5]
surveyanswers <- aggregate[,-which(names(aggregate) %in% names(personinfo))]
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("LT1a _Confidence",colnames(surveyanswers)))] <- "LT1a_Confidence"
colnames(surveyanswers)[c(grep("LT5c_Contributes.1",colnames(surveyanswers)))] <- "LT56c_Confidence"
data.LT <- surveyanswers[,c(grep("LT",colnames(surveyanswers)))]
data.LT <- data.LT[,c(grep("Confidence",colnames(data.LT)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
LT.ordered <- data.LT
LT.numer <- data.LT
for (i in 1:ncol(data.LT)) {
  LT.ordered[,i] <- factor(data.LT[,i],levels=ans)
  LT.numer[,i] <- as.numeric(LT.ordered[,i])
}
colnames(LT.numer) <- colnames(data.LT)
```


```{r, echo=FALSE}
survey <- cbind(personinfo,surveyanswers)
colnames(surveyanswers)[c(grep("PL1a _Confidence",colnames(surveyanswers)))] <- "PL1a_Confidence"
colnames(surveyanswers)[c(grep("PL6c_Contributes.1",colnames(surveyanswers)))] <- "PL6c_Confidence"

data.PL <- surveyanswers[,c(grep("PL",colnames(surveyanswers)))]
data.PL <- data.PL[,c(grep("Confidence",colnames(data.PL)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
PL.ordered <- data.PL
PL.numer <- data.PL
for (i in 1:ncol(data.PL)) {
  PL.ordered[,i] <- factor(data.PL[,i],levels=ans)
  PL.numer[,i] <- as.numeric(PL.ordered[,i])
}
colnames(PL.numer) <- colnames(data.PL)


PL1 <- 'pl1 =~ PL1a_Confidence + PL1b_Confidence + PL1c_Confidence + PL1d_Confidence + PL1f_Confidence
PL1a_Confidence ~~ PL1f_Confidence'
fit1 <- cfa(PL1,data=PL.numer,std.lv=T) 
PL1table <- c("PL1","PL1a,PL1b,PL1c,PL1d,PL1f",0.887, round(fitMeasures(fit1)[c("cfi")],digits=3),round(fitMeasures(fit1)[c("tli")],digits=3))


PL2 <- 'pl2 =~ aa*PL2a_Confidence + aa*PL2b_Confidence + PL2c_Confidence'
fit2 <- cfa(PL2,data=PL.numer,std.lv=T) #EStimate Error Variances
PL2table <- c("PL2","PL2a,PL2b,PL2c",0.292, round(fitMeasures(fit2)[c("cfi")],digits=3),round(fitMeasures(fit2)[c("tli")],digits=3))

PL3 <- 'pl3 =~ PL3a_Confidence + aa*PL3d_Confidence + aa*PL3e_Confidence'
fit3 <- cfa(PL3,data=PL.numer,std.lv=T) #EStimate Error Variances
PL3table <- c("PL3","PL3a,PL3d,PL3e",0.902, round(fitMeasures(fit3)[c("cfi")],digits=3),round(fitMeasures(fit3)[c("tli")],digits=3))


PL4 <- 'pl4 =~ aa*PL4a_Confidence + aa*PL4b_Confidence + PL4c_Confidence'
fit4 <- cfa(PL4,data=PL.numer,std.lv=T) #EStimate Error Variances
PL4table <- c("PL4","PL4a,PL4b,PL4c",0.051, round(fitMeasures(fit4)[c("cfi")],digits=3),round(fitMeasures(fit4)[c("tli")],digits=3))

PL5 <- 'pl5 =~ PL5a_Confidence + PL5b_Confidence + PL5c_Confidence + PL5d_Confidence'
fit5 <- cfa(PL5,data=PL.numer,std.lv=T) #EStimate Error Variances
PL5table <- c("PL5","PL5a,PL5b,PL5c,PL5d",0.261, round(fitMeasures(fit5)[c("cfi")],digits=3),round(fitMeasures(fit5)[c("tli")],digits=3))


PL6 <- 'pl6 =~ aa*PL6a_Confidence + aa*PL6b_Confidence + PL6c_Confidence'
fit6 <- cfa(PL6,data=PL.numer,std.lv=T) #EStimate Error Variances
PL6table <- c("PL6","PL6a,PL6b,PL6c",0.283, round(fitMeasures(fit6)[c("cfi")],digits=3),round(fitMeasures(fit6)[c("tli")],digits=3))

PL7 <- 'pl7 =~ aa*PL7a_Confidence + PL7b_Confidence + aa*PL7c_Confidence'
fit7 <- cfa(PL7,data=PL.numer,std.lv=T) #EStimate Error Variances
PL7table <- c("PL7","PL7a,PL7b,PL7c",0.903, round(fitMeasures(fit7)[c("cfi")],digits=3),round(fitMeasures(fit7)[c("tli")],digits=3))


PL8 <- 'pl8 =~ PL8a_Confidence + aa*PL8b_Confidence + aa*PL8c_Confidence'
fit8 <- cfa(PL8,data=PL.numer,std.lv=T) #EStimate Error Variances
PL8table <- c("PL8","PL8a,PL8b,PL8c",0.301, round(fitMeasures(fit8)[c("cfi")],digits=3),round(fitMeasures(fit8)[c("tli")],digits=3))
```

## Planning Domain
```{r,echo=F}
PLTable <- rbind(PL1table,PL2table,PL3table,PL4table,PL5table,PL6table,PL7table,PL8table)
colnames(PLTable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(PLTable) <- NULL
kable(PLTable,digits=3,format="pandoc",booktabs=T,caption="'Planning' Subdomain Summary")
```

Summary statistics for the subdomains of PL1 are shown in Table 1. Questions were removed based on our protocol, and the remaining questions are shown in the “Questions” table. Questions PL1e, PL1g, PL3b,PL3c, PL6d, and PL8d were removed. All additional questions were found to not fit well within the model, and may need to be treated separately.
The models meet the gold standard of a Comparative Fit Index (CFI) of 0.90, indicating that there is not a major discrepancy between the hypothetical models and the data. The Tucker-Lewis Index (TLI) for each model are also close or lower to 1, supporting that the data and models seem to be close. 
The P-values for each of the model all are relatively high, indicating that they most likely follow the null hypothesis. Effectively, this means that the questions within the model can be grouped into their subdomain. PL4 may be the only exception, since it has a P-value close to 0.051. However, the CFI and TLI of the model remain high, so it may be correct to use it as one model.


## Learning Tool Domain
```{r, include=F}
data.LT <- surveyanswers[,c(grep("LT",colnames(surveyanswers)))]
data.LT <- data.LT[,c(grep("Confidence",colnames(data.LT)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
LT.ordered <- data.LT
LT.numer <- data.LT
for (i in 1:ncol(data.LT)) {
  LT.ordered[,i] <- factor(data.LT[,i],levels=ans)
  LT.numer[,i] <- as.numeric(LT.ordered[,i])
}
colnames(LT.numer) <- colnames(data.LT)

LT1 <- 'lt1 =~ aa*LT1a_Confidence + aa*LT1b_Confidence + LT1c_Confidence '
fit1 <- cfa(LT1,data=LT.numer,std.lv=T)
LT1table <- c("LT1","LT1a,LT1b,LT1c",0.741, round(fitMeasures(fit1)[c("cfi")],digits=3),round(fitMeasures(fit1)[c("tli")],digits=3))

LT2 <- 'lt2 =~ aa*LT2a_Confidence + aa*LT2b_Confidence + LT2c_Confidence'
fit2 <- cfa(LT2,data=LT.numer,std.lv=T)
LT2table <- c("LT2","LT2a,LT2b,LT2c",0.953, round(fitMeasures(fit2)[c("cfi")],digits=3),round(fitMeasures(fit2)[c("tli")],digits=3))


LT3 <- 'lt3 =~ LT3a_Confidence + aa*LT3b_Confidence + aa*LT3d_Confidence'
fit3 <- cfa(LT3,data=LT.numer,std.lv=T)
LT3table <- c("LT3","LT3a,LT3b,LT3d",0.897, round(fitMeasures(fit3)[c("cfi")],digits=3),round(fitMeasures(fit3)[c("tli")],digits=3))



LT4 <- 'lt4 =~ LT4a_Confidence + aa*LT4b_Confidence + aa*LT4c_Confidence'
fit4 <- cfa(LT4,data=LT.numer,std.lv=T)
LT4table <- c("LT4","LT4a,LT4b,LT4c",0.899, round(fitMeasures(fit4)[c("cfi")],digits=3),round(fitMeasures(fit4)[c("tli")],digits=3))


LT5 <- 'lt5 =~ aa*LT5a_Confidence + LT5b_Confidence + aa*LT5c_Confidence'
fit5 <- cfa(LT5,data=LT.numer,std.lv=T)
LT5table <- c("LT5","LT5a,LT5b,LT5c",0.379, round(fitMeasures(fit5)[c("cfi")],digits=3),round(fitMeasures(fit5)[c("tli")],digits=3))
```

```{r,echo=F}
LTTable <- rbind(LT1table,LT2table,LT3table,LT4table,LT5table)
colnames(LTTable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(LTTable) <- NULL
kable(LTTable,digits=3,format="pandoc",caption="'Learning Tools' Subdomain Summary")
```

The summary statistics for the Learning Tools subdomains are shown in Table 2. As before, questions included in each subdomain are listed in the "Questions" column. The only question removed due to the protocal was LT3c, which may need to be treated separately.  The CFI and TLI both seem high and close to 1 respectively, showing that the data and proposed models are relatively close. LT1 may need to be considered more closely, since its LT1 is relatively larger than the rest of these values. However, it still seems to show a relatively close comparison between the data and proposed models.
Once again, our p-values indicate that the null hypothesis cannot be rejected, and the questions can effectively be grouped into a subdomain.

## Per & Feedback Domain

```{r,echo=FALSE}
# First subdomain
data.PF <- surveyanswers[,c(grep("PF",colnames(surveyanswers)))]
data.PF <- data.PF[,c(grep("Confidence",colnames(data.PF)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
PF.ordered <- data.PF
PF.number <- data.PF
for (i in 1:ncol(data.PF)) {
  PF.ordered[,i] <- factor(data.PF[,i],levels=ans)
  PF.number[,i] <- as.numeric(PF.ordered[,i])
}
colnames(PF.number) <- colnames(data.PF)


m1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence + PF1e_Confidence'
fit1 <- cfa(m1,data = PF.number, std.lv=TRUE)

m1.1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence'
fit1.1 <- cfa(m1.1,data = PF.number, std.lv=TRUE)
PF1table <- c("PF1","PF1a,PF1b,PF1c,PF1d",0.967, round(fitMeasures(fit1.1)[c("cfi")],digits=3),round(fitMeasures(fit1.1)[c("tli")],digits=3))


# Second subdomain
m2.1 <- 'PF2 =~ PF2a_Confidence + PF2b_Confidence + PF2d_Confidence + PF2e_Confidence'
fit2.1 <- cfa(m2.1,data = PF.number, std.lv=TRUE)
PF2table <- c("PF2","PF2a,PF2b,PF2d,PF2e",0.459, round(fitMeasures(fit2.1)[c("cfi")],digits=3),round(fitMeasures(fit2.1)[c("tli")],digits=3))


# Third subdomain
m3 <- 'PF3 =~ PF3a_Confidence + PF3b_Confidence + PF3c_Confidence + PF3d_Confidence + PF3e_Confidence'
fit3 <- cfa(m3,data = PF.number, std.lv=TRUE)
PF3table <- c("PF3","PF3a,PF3b,PF3c,PF3d,PF3e",0.485, round(fitMeasures(fit3)[c("cfi")],digits=3),round(fitMeasures(fit3)[c("tli")],digits=3))



# Fourth subdomain
m4 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence + PF5c_Confidence'
fit4 <- cfa(m4,data = PF.number, std.lv=TRUE)

m4.1 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence'
fit4.1 <- cfa(m4.1,data = PF.number, std.lv=TRUE)
PF4table <- c("PF4 & PF5","PF4a,PF4b,PF5a,PF5b",0.362, round(fitMeasures(fit4.1)[c("cfi")],digits=3),round(fitMeasures(fit4.1)[c("tli")],digits=3))
```

```{r,echo=F}
PFTable <- rbind(PF1table,PF2table,PF3table,PF4table)
colnames(PFTable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(PFTable) <- NULL
kable(PFTable,digits=3,format="pandoc",caption="'Performance & Feedback' Subdomain Summary")

```

The summary statistics for the Performance and Feedback subdomain is shown in Table 3. Excluded questions from our protocol were PF1e, PF2c, and PF5c. Since PF4 only contained two questions, following our protocol, it was treated in combination with PL5 in order to be assessed with our CFA method. 

Once again, the calculated CFI and TLI are above 0.9 and close to 1 respectively, indicating that the data and proposed models follow each other well. Additionally, p-values are higher than the 0.05 threshold, indicating that these subdomains can be used to group questions together effectively.

## Learning Experience Domain

```{r,echo=FALSE}
PL.factor <- select(survey,'LE1a_Contributes':'LE6d_Confidence')
PL.factor <-lapply(PL.factor,function(x) as.numeric(as.factor(x)))
PL.factor <- as.data.frame(PL.factor)

LE1model<-'LE1 =~LE1a_Confidence+LE1b_Confidence+LE1c_Confidence+LE1d_Confidence'
fitLE1 <- cfa(LE1model,data=PL.factor,std.lv=T)
LE1table <- c("LE1","LE1a,LE1b,LE1c,LE1d",0.672, round(fitMeasures(fitLE1)[c("cfi")],digits=3),round(fitMeasures(fitLE1)[c("tli")],digits=3))

LE2model<-'LE2 =~LE2a_Confidence+LE2c_Confidence+LE2d_Confidence+LE2f_Confidence'
fitLE2 <- cfa(LE2model,data=PL.factor,std.lv=T)
LE2table <- c("LE2","LE2a,LE2c,LE2d,LE2f",0.412, round(fitMeasures(fitLE2)[c("cfi")],digits=3),round(fitMeasures(fitLE2)[c("tli")],digits=3))


LE3model<-'LE3 =~LE3a_Confidence+LE3b_Confidence+LE3d_Confidence+LE3e_Confidence+LE3f_Confidence'
fitLE3 <- cfa(LE3model,data=PL.factor,std.lv=T)
LE3table <- c("LE3","LE3a,LE3b,LE3d,LE3e,LE3f",0.951, round(fitMeasures(fitLE3)[c("cfi")],digits=3),round(fitMeasures(fitLE3)[c("tli")],digits=3))


LE4model<-'LE4 =~LE4a_Confidence+LE4b_Confidence+LE4c_Confidence+LE4d_Confidence'
fitLE4 <- cfa(LE4model,data=PL.factor,std.lv=T)
LE4table <- c("LE4","LE4a,LE4b,LE4c,LE4d",0.13, round(fitMeasures(fitLE4)[c("cfi")],digits=3),round(fitMeasures(fitLE4)[c("tli")],digits=3))


LE5model<-'LE5 =~aa*LE5a_Confidence+aa*LE5c_Confidence+LE5d_Confidence'
fitLE5 <- cfa(LE5model,data=PL.factor,std.lv=T)
LE5table <- c("LE5","LE5a,LE5c,LE5d",0.857, round(fitMeasures(fitLE5)[c("cfi")],digits=3),round(fitMeasures(fitLE5)[c("tli")],digits=3))


LE6model<-'LE6 =~aa*LE6b_Confidence+LE6c_Confidence+aa*LE6d_Confidence'
fitLE6 <- cfa(LE6model,data=PL.factor,std.lv=T)
LE6table <- c("LE6","LE6b,LE6c,LE6d",0.657, round(fitMeasures(fitLE6)[c("cfi")],digits=3),round(fitMeasures(fitLE6)[c("tli")],digits=3))
```

```{r,echo=F}
LETable <- rbind(LE1table,LE2table,LE3table,LE4table,LE5table,LE6table)
colnames(LETable) <- c("Section","Questions","P-Value","CFI","TLI")
rownames(LETable) <- NULL
kable(LETable,digits=3,format="pandoc",caption="'Learning Experience' Subdomain Summary")

```

The results for the Learning Experience Domain can be shown in Table 4. The questions removed due to the question removal protocol are LE1a, LE2b, LE2e, LE3c, LE3g, LE4e, LE5b, and LE6a. These questions may need to be treated separately when restructuring the survey.

Our CFI and TLI values are both high and close to 1, indicating that the models fit the data. The TLI for LE3 is relatively higher than the rest, which may mean it needs to be considered separately. However, it is still relatively close to 1, and still indicates a decent fit between data and model. The p-values are above our threshold of 0.05, indicating that each one groups each set of questions well.

# 5. Conclusion

In this report, we have proposed a structure to group and remove large set of question based on the structure of the TELL Framework. In our analysis, we used a Confirmatory Factor Analysis to show that many of the survey questions can be grouped in a larger structure. This may highlight a method to reduce question number, where, rather than asking each of the questions, one question is asked for each group. However, this will require a change in questioning and possibly a change in structure.

The questions removed from the subdomains must be considered separately. Usually, they were removed because the way they were answered followed a significantly different pattern from other questions. There may be a final structure that does group these with the rest. Our analysis only shows that they don't fit best under the groupings provided by the TELL framework.

## Appendix

## Learning Tool Domain Analysis
For Learning Tools table in TELL Statements, we numeric character answers of LT 1a~5c Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LT1 has 3 variables: LT1a_Confidence, LT1b_Confidence, LT1c_Confidence).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results:
# First subdomain:
```{r echo=FALSE, message=FALSE, warning=FALSE}
LT1 <- 'lt1 =~ aa*LT1a_Confidence + aa*LT1b_Confidence + LT1c_Confidence '
fit1 <- cfa(LT1,data=LT.numer,std.lv=T)
summary(fit1,standardized=T)
parameterEstimates(fit1,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.741>0.05. Factor loadings: a-0.488;b-0.472;c-0.722. cfi=1;ltli=1.607.
```

Since p-value of the first subdomain is 0.741 > 0.05, there is no need to make any change in the first subdomain and we can save all questions. 

# Second subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT2 <- 'lt2 =~ aa*LT2a_Confidence + aa*LT2b_Confidence + LT2c_Confidence'
fit2 <- cfa(LT2,data=LT.numer,std.lv=T)
summary(fit2,standardized=T)
parameterEstimates(fit2,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.953>0.05. Factor loadings: a-0.587;b-0.603;c-0.817. cfi=1; tli=1.234.
```

Since p-value of the second subdomain is 0.953 > 0.05, there is no need to make any change in the second subdomain and we can save all questions. 

# Third subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT3 <- 'lt3 =~ LT3a_Confidence + LT3b_Confidence + LT3c_Confidence + LT3d_Confidence'
fit3 <- cfa(LT3,data=LT.numer,std.lv=T)
summary(fit3,standardized=T)
parameterEstimates(fit3,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Initial p-value with 4 questions is 0.008. 
```

Since p-value of the third subdomain is 0.008 < 0.05, and question "LT3c_Confidence" has the lowest factor loading 0.604, we drop "LT3_c_Confidence" and then remodel the third subdomain. 

```{r echo=FALSE, message=FALSE, warning=FALSE }
LT3_new <- 'lt3 =~ LT3a_Confidence + aa*LT3b_Confidence + aa*LT3d_Confidence'
fit3new <- cfa(LT3_new,data=LT.numer,std.lv=T)
summary(fit3new,standardized=T)
parameterEstimates(fit3new,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Initial p-value with 4 questions is 0.008. 
#After dropping question c, the p-value=0.897. Factor loadings:a-0.959;b-0.802;d-0.629. cfi=1;tli=1.087.
```

After we remodel the third subdomain, the p-value of third domain is 0.897 > 0.05. Then we can save all the remaining questions in the third subdomain ("LT3a_Confidence", "LT3b_Confidence", "LT3d_Confidence"). 

# Fourth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT4 <- 'lt4 =~ LT4a_Confidence + aa*LT4b_Confidence + aa*LT4c_Confidence'
fit4 <- cfa(LT4,data=LT.numer,std.lv=T)
summary(fit4,standardized=T)
parameterEstimates(fit4,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.899. Factor loadings: a-0.469;b-1.012;c-0.886. cfi=1; tli=1.06.
```

Since p-value of the fourth subdomain is 0.899 > 0.05, there is no need to make any change in the fourth subdomain and we can save all questions. 

# Fifth subdomain
```{r echo=FALSE, message=FALSE, warning=FALSE }
LT5 <- 'lt5 =~ aa*LT5a_Confidence + LT5b_Confidence + aa*LT5c_Confidence'
fit5 <- cfa(LT5,data=LT.numer,std.lv=T)
summary(fit5,standardized=T)
parameterEstimates(fit5,standardized=T) %>%
  filter(op=="=~") %>%
  select('Latent Factor'=lhs,Indicator=rhs,B=est,SE=se,Z=z,'p-value'=pvalue,loading=std.all) %>%
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#P-value=0.379. Factor loadings: a-0.618;b-0.844;c-0.532. cfi=1; tli=1.055.
```

Since p-value of the fifth subdomain is 0.379 > 0.05, there is no need to make any change in the fifth subdomain and we can save all questions. 

## PER & FEEDBACK Domain Analysis
For PER&FEEDBACK table in TELL Statements, I numeric character answers of PF 1a~5c Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain whose variables should greater than 2 (ex: PF1 has 5 variables: PF1a_Confidence, PF1b_Confidence, PF1c_Confidence,PF1d_Confidence and PF1e_Confidence), or the P-value of that model will become NA. And we get an exception in PF table: PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we have an available P-value. Second, we find factor loadings of each variables in each subdomain and record them. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again.
Following are detailed results   
# First subdomain:
```{r, echo=FALSE}
colnames(surveyanswers)[c(grep("PF1a _Confidence",colnames(surveyanswers)))] <- "PF1a_Confidence"
colnames(surveyanswers)[c(grep("PF5c_Contributes.1",colnames(surveyanswers)))] <- "PF5c_Confidence"
data.PF <- surveyanswers[,c(grep("PF",colnames(surveyanswers)))]
data.PF <- data.PF[,c(grep("Confidence",colnames(data.PF)))]
ans = c("I'm not sure what this means", "Not confident at all","Somewhat confident","Neutral","Confident","Very Confident")
PF.ordered <- data.PF
PF.number <- data.PF
for (i in 1:ncol(data.PF)) {
  PF.ordered[,i] <- factor(data.PF[,i],levels=ans)
  PF.number[,i] <- as.numeric(PF.ordered[,i])
}
colnames(PF.number) <- colnames(data.PF)
# First subdomain
m1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence + PF1e_Confidence'
fit1 <- cfa(m1,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit1,standardized=T)
#P-value = 0.008
parameterEstimates(fit1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.609; 0.830; 0.946; 0.915; 0.587
```


Since p-value of first subdomain is 0.008 < 0.05, and the factor loadings of "PF1e_Confidence" is lowest, thus, we try to drop it from the first subdomain: 
```{r, echo=FALSE}
m1.1 <- 'PF1 =~ PF1a_Confidence + PF1b_Confidence + PF1c_Confidence + PF1d_Confidence'
fit1.1 <- cfa(m1.1,data = PF.number, std.lv=TRUE)
summary(fit1.1,standardized=T)
```
P-value = 0.967 > 0.05, thus we do not need to change any more on the first subdomain.

# Second subdomain:
```{r, echo=FALSE}
# Second subdomain
m2 <- 'PF2 =~ PF2a_Confidence + PF2b_Confidence + PF2c_Confidence + PF2d_Confidence + PF2e_Confidence'
fit2 <- cfa(m2,data = PF.number, std.lv=TRUE)
summary(fit2,standardized=T)
#P-value = 0.013
parameterEstimates(fit2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.587; 0.905; 0.558; 0.901; 0.896
```

Since p-value of first subdomain is 0.013 < 0.05, and the factor loadings of "PF2c_Confidence" is lowest, thus, we try to drop it from the second subdomain:
```{r, echo=FALSE}
m2.1 <- m2 <- 'PF2 =~ PF2a_Confidence + PF2b_Confidence + PF2d_Confidence + PF2e_Confidence'
fit2.1 <- cfa(m2.1,data = PF.number, std.lv=TRUE)
summary(fit2.1,standardized=T)
```
P-value = 0.459 > 0.05, thus we can stay here for the second subdomain.


# Third subdomain:
```{r, echo=FALSE}
m3 <- 'PF3 =~ PF3a_Confidence + PF3b_Confidence + PF3c_Confidence + PF3d_Confidence + PF3e_Confidence'
fit3 <- cfa(m3,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit3,standardized=T)
#P-value = 0.712 
parameterEstimates(fit3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.485; 0.838; 0.746; 0.767; 0.588
```
Since p-value > 0.05, the third subdomain is ok, no longer to remodel it.


# Fourth subdomain:
PF4 only has 2 varaibles, so I combine PF4 with PF5 to one CFA model so that we can get an available P-value.
```{r, echo=FALSE}
m4 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence + PF5c_Confidence'
fit4 <- cfa(m4,data = PF.number, std.lv=TRUE)
#summary(fit1,fit.measures=TRUE,standardized=TRUE)
summary(fit4,standardized=T)
#P-value = 0.012 
parameterEstimates(fit4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
#Factor loadings: 0.888; 0.835; 0.757; 0.759; 0.567
```

Since P-value is 0.012 < 0.05, and the lowest factor loading is "PF5c_Confidence", thus we try to drop it from the subdomain:
```{r, echo=FALSE}
m4.1 <- 'PF4 =~ PF4a_Confidence + PF4b_Confidence
       PF5 =~ PF5a_Confidence + PF5b_Confidence'
fit4.1 <- cfa(m4.1,data = PF.number, std.lv=TRUE)
summary(fit4.1,standardized=T)
```
P-value is 0.362 > 0.05, thus no longer remodel this subdomain.

## Learning Experience Domain Analysis
For learning experience table in TELL Statements, we numeric character answers of LE 1a~6d Confidence, and NA values stay as same as NA that will not count in. First, I made CFA models for each subdomain (ex: LE1 has 5 variables: LE1a_Confidence, LE1b_Confidence, LE1c_Confidence,LE1d_Confidence and LE1e_Confidence).Then we have an available P-value for each subdomain and we find factor loadings of each variables in each subdomain. Third, we compare P-value of each subdomain to 0.05, if P-value > 0.05, our null hypothesis retained, and we do not need to make any further change on that subdomain; if P-value < 0.05, it means our null hypothesis is rejected, and we need to remodel by droping the variable with lowest factor loadings in that subdomain and check its P-value again. Following are detailed results

## First subdomian
```{r,echo=F}
PL.factor <- select(survey,'LE1a_Contributes':'LE6d_Confidence')
PL.factor <-lapply(PL.factor,function(x) as.numeric(as.factor(x)))
PL.factor <- as.data.frame(PL.factor)
LE1model<-'LE1 =~LE1a_Confidence+LE1b_Confidence+LE1c_Confidence+LE1d_Confidence+LE1e_Confidence'
fitLE1 <- cfa(LE1model,data=PL.factor)
summary(fitLE1)
parameterEstimates(fitLE1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

The p-value of this subdomian is 0.762, so we will keep all the questions in this subdomian.

## Second Subdomain
```{r,echo=F}
LE2model<-'LE2 =~LE2a_Confidence+LE2b_Confidence+LE2c_Confidence+LE2d_Confidence+LE2e_Confidence+LE2f_Confidence'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the second subdomain, the p-value is 0.028<0.05, so we will drop the question LE2a to see how the model will be.
```{r,echo=F}
LE2model<-'LE2 =~LE2b_Confidence+LE2c_Confidence+LE2d_Confidence+LE2e_Confidence+LE2f_Confidence'
fitLE2 <- cfa(LE2model,data=PL.factor)
summary(fitLE2)
parameterEstimates(fitLE2, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

After dropping the LE2a, we have a p value of 0.9>0.05. So we will keep all the other questions.

## Third Subdomain
```{r,echo=F}
LE3model<-'LE3 =~LE3a_Confidence+LE3b_Confidence+LE3c_Confidence+LE3d_Confidence+LE3e_Confidence+LE3f_Confidence+LE3g_Confidence'
fitLE3 <- cfa(LE3model,data=PL.factor)
summary(fitLE3)
parameterEstimates(fitLE3, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the third subdomian,we have a p value of 0.117>0.05, so we will keep all the questions.

## Fourth Subdomain
```{r,echo=F}
LE4model<-'LE4 =~LE4a_Confidence+LE4b_Confidence+LE4c_Confidence+LE4d_Confidence+LE4e_Confidence'
fitLE4 <- cfa(LE4model,data=PL.factor)
summary(fitLE4)
parameterEstimates(fitLE4, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the fourth subdomain, we have a p value of 0.153. We will keep all the questions in this subdomain.

## Fifth subdomain
```{r,echo=F}
LE5model<-'LE5 =~LE5a_Confidence+LE5b_Confidence+LE5c_Confidence+LE5d_Confidence'
fitLE5 <- cfa(LE5model,data=PL.factor)
summary(fitLE5)
parameterEstimates(fitLE5, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the fifth subdomain, we have a p-value of 0.123, so we will keep all the questions in this dubdomain.

## Sixth subdomain
```{r,echo=F}
LE6model<-'LE6 =~LE6a_Confidence+LE6b_Confidence+LE6c_Confidence+LE6d_Confidence'
fitLE6 <- cfa(LE6model,data=PL.factor)
summary(fitLE6)
parameterEstimates(fitLE6, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, loading=std.all) %>% 
  kable(digits = 3, format="pandoc", caption="Factor Loadings")
```

In the sixth subdomain, the p-value is 0.66>0.05. We will not drop any question in this subdomain.

